<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="UTF-8" />

    <meta content="width=device-width,initial-scale=1" name="viewport" />

    <link href="styles.css" rel="stylesheet" />

    <title>IE Seminar - Fine-tuning a BERT model</title>
  </head>

  <body><header> <a href="index.html"> « IE Seminar</a> </header><section><div
  class="main-title"><h1>BERT Model 2: Fine-Tuning Deep-Learning Model for
  Passage Boundary Detection</h1><p><strong>Institute of Computer Science,
  Brandenburgische Technische Universität Cottbus-Senftenberg</strong><br />
  Juan-Francisco Reyes<br />
  pacoreyes@protonmail.com</p></div><main><section><p style="color: red">###
  THIS PAGE WILL BE UPDATED PERMANENTLY BASED ON INTERACTIONS ON THE FORUM,
  RETURN OFTEN</p><h3>Overview</h3><p>This document delineates the process of
  preprocessing Dataset 2 and fine-tuning a BERT model for binary text
  classification.</p><h3>Challenges in Passage Boundary
  Detection</h3><p>Passage Boundary Detection (PBD) refers to identifying the
  boundaries between distinct passages or segments within a text, which is
  essential for tasks such as document summarization, information retrieval,
  and text segmentation, where understanding the structure of the text is
  crucial. The main challenges in PSB are:</p><ol>
      <li><p><strong>Definition agreement</strong>: There is no universal
      agreement on what constitutes a "passage," as its definition can vary
      significantly depending on the type of text (e.g., literary texts,
      scientific articles, news reports, political discourse) and the specific
      application. This variability adds complexity to designing algorithms
      that can universally detect passage boundaries.</p></li>

      <li><p><strong>Ambiguity</strong>: Unlike sentences often delimited by
      clear punctuation marks, passage boundaries can be vague and unclear.
      Identifying where one passage ends and another begins requires
      understanding the linguistic features that link sentences, context, the
      topic, and sometimes the subtle topic shifts.</p></li>

      <li><p><strong>Continuity</strong>: Passages may have thematic or
      contextual overlaps, making determining where one topic ends and another
      begins is challenging. In fine-grained PBD, distinguishing these subtle
      changes in topic or perspective requires a sophisticated understanding
      and analysis of the text.</p></li>

      <li><p><strong>Variability</strong>: Variations in writing style,
      language use, and narrative techniques across different authors and text
      types can impact the effectiveness of PBD algorithms. Detecting
      boundaries in more stylistically complex or linguistically diverse texts
      can be particularly challenging.</p></li>

      <li><p><strong>Advanced NLP</strong>: Effective fine-grained PBD often
      requires advanced NLP techniques to understand nuanced language patterns
      and contextual cues. However, overall, these techniques require mainly
      well-annotated datasets for training.</p></li>

      <li><p><strong>Linguistic analysis</strong>: Understanding coherence
      (logical flow and connection between ideas) and cohesion (the
      grammatical and lexical linking within a text) is vital for effective
      PBD, allowing the model to process the text at a surface level and
      understand deeper linguistic and semantic structures.</p></li>
    </ol><h3>Annotation Approach: Pair Labelling</h3><p>This approach focuses
  on analyzing and annotating pairs of sentences to determine how they relate
  to each other in terms of continuity and coherence. This process is
  fundamental in understanding the flow of ideas and logical progression in a
  text.</p><p>Typically, the annotation schema includes binary labels like
  "Same Topic" and "Topic Change". Annotators review pairs of sentences and
  assign one of these labels based on the thematic continuity or
  discontinuity. We will use the labels "continue" and
  "not_continue".</p><p>Example:</p><ol>
      <li><p><strong>[not_continue]</strong> And I want to thank Mike. Today,
      it's a true honor to be the first President of the United States to host
      a meeting at the United Nations on religious freedom.</p></li>

      <li><p><strong>[continue]</strong> Today, it's a true honor to be the
      first President of the United States to host a meeting at the United
      Nations on religious freedom. And an honor it is. It's long
      overdue.</p></li>

      <li><p><strong>[continue]</strong> And an honor it is. It's long
      overdue. And I was shocked when I was given that statistic that I would
      be the first.</p></li>

      <li>(...)</li>
    </ol><p>The core idea is to analyze pairs of adjacent sentences and label
  them based on whether they continue the same topic or start a new one. This
  binary classification (same topic vs. new topic) helps identify the points
  where the subject matter shifts, thereby detecting topic
  boundaries.</p><p>In this project, the annotation will be made automatically
  from the annotated passages from Dataset 2. </p><h3>Files and folders
  structure</h3><p>This is the list of provided files:</p><ul>
      <li><p><kbd>build_dataset_baseline.py</kbd>: Script to convert annotated
      data into dataset 2 for the pair sentence labelling approach.</p></li>

      <li><p><kbd>dl_bert_train.py</kbd>: Script to fine-tune a BERT model for
      sentence boundary detection using the pair sentence labelling
      approach.</p></li>

      <li><kbd>dataset2_raw.jsonl</kbd>: Downloaded from the Annotation Tool
      on Monday, December 11, 2023, at 10:59. For reproducibility of the model
      baseline results described in this document.</li>
    </ul><h3>"Feature Engineering" and BERT Models</h3><p>In classical machine
  learning models, feature engineering is a critical step involving selecting
  manually and transforming raw data into a set of features that the model can
  understand and process effectively. In contrast, Transformer models,
  including BERT, are designed to automatically learn feature representations
  from raw text data through multiple layers of self-attention and feedforward
  neural networks. This process allows the model to learn complex
  representations of the input text without explicit manual intervention in
  the feature extraction process.</p><p>While fine-tuning BERT models does not
  entail manually selecting or creating features, it involves selecting
  appropriate datasets that are representative of the task at hand. For NLP
  projects, this involves understanding linguistic structures and features in
  the classification challenge. The diversity, representation, and quantity of
  datapoints are crucial; thus, curating a dataset that effectively represents
  the problem space and contains various examples for the model to learn from
  is essential. Additionally, converting raw text into a format for BERT
  models is another critical step. This process involves tokenization
  (breaking text into tokens), adding special tokens (like <kbd>[CLS]</kbd>
  and <kbd>[SEP]</kbd>), and creating attention masks. Finally, determining
  appropriate hyperparameters (such as learning rate, batch size, etc.) can
  also be considered a part of feature engineering in this context, which
  involves tuning the model to fit the specific characteristics of the data
  and the task better.</p><p>While feature engineering in the traditional
  sense is not a primary concern when working with BERT and similar models,
  the selection and preparation of data, along with model configuration and
  hyperparameter tuning, play a crucial role in the fine-tuning process. These
  steps ensure the model learns the most relevant features from the data for
  the specific NLP task.</p><h3>Before You Begin</h3><p>Do "feature
  engineering" by completing first the peer review of the dataset according to
  the table of reviewer/reviewed. Every improvement in the dataset will impact
  the overall result. For this purpose, you will follow these steps:</p><ol
      type="a">
      <li><p>Complete a last review of the annotation of passages ASAP, and
      contact your reviewers to let them know when they may start their peer
      review task. You can also coordinate with your reviewer to do reviews in
      batches according to your progress - that requires high organization
      between parties. <span style="color: red">The deadline for the peer
      review has been extended from December 8 to December 13 at 6:00. From
      that point, Dataset 2 will be downloaded and the grading process for
      dataset building (3 points) and dataset peer review (2 points) will
      begin; therefore, no change should be made to the dataset
      afterward</span>.</p></li>

      <li><p>Every wrong passage found in the peer review must be flagged as
      "to reject" in the IIA document, and the annotator (not the reviewer)
      must correct the mistake or reject it. Recall that the model accuracy
      improves whenever a wrong passage is corrected or rejected. Follow what
      Mr Urrego has done by logging and rejecting wrong datapoints in Mr
      Shresda's annotation on the shared Google Spreadsheet.</p></li>

      <li><p>Announce on the forum when you finish your peer-review task to
      let your classmates know that an improved version of Dataset 2 is
      available to download. Effective communication and collaborative work
      are crucial and will be evaluated in Project 2's individual grade. See
      grading schema.</p></li>
    </ol><p>In the real world, building a dataset is a collaborative project;
  therefore, any individual contribution to improve the dataset will impact
  the model's performance and, in this case, the grade of every participant.
  Work collaboratively!!</p><p>Notice that in this project, we are trying to
  capture nuanced linguistic features that define topic shifts in political
  discourse. Likely, the number of examples (datapoints) to fine-tune a BERT
  model will not be enough to get a high performance. For this reason, in this
  complex project, we are not going to be very ambitious with performance
  metrics.</p><h3>Procedure</h3><ol>
      <li>The two factors in which you have full control and have the higher
      impact on the final results are the dataset quality and training
      hyperparameters. Therefore, don't expect a higher performance in your
      model if your annotation work is poor.</li>

      <li><p>Download Dataset 2 JSON file (<kbd>dataset2_raw.jsonl</kbd>) from
      the annotation tool.</p></li>

      <li><p>Use the script
      <kbd><kbd>build_dataset_sent_pair_labelling.py</kbd></kbd> to generate
      the annotated dataset for the sentence pair labelling annotation
      approach. The text will be automatically anonymized. A new version of
      your dataset will be generated with the name
      <kbd>dataset2_pair_sentence_labelling.jsonl</kbd>.</p></li>

      <li>Study the model baseline and grading schema.</li>

      <li><p>Modify hyperparameters iteratively to achieve the highest
      accuracy possible of the model. See grading schema to see how your grade
      is calculated.</p></li>

      <li><p>Use the visualizations to evaluate the performance of the model.
      The <a
      href="https://www.evidentlyai.com/classification-metrics/confusion-matrix"
      target="_blank">confusion matrix</a> visualization must be generated
      (<kbd>confusion_matrix_sent_pair_labelling.png</kbd>) to evaluate the
      performance of the model by visualizing False and True Positives and
      Negatives in two dimensions, "Actual" and "Predicted". Take the code
      from Project 1 to generate a confusion matrix visualization. The
      training and validation losses visualization
      (<kbd>losses_sent_pair_labelling.png</kbd>) to follow up if your model
      is overfitting or underfitting [<a
      href="https://medium.com/nlplanet/bert-finetuning-with-hugging-face-and-training-visualizations-with-tensorboard-46368a57fc97"
      target="_blank">link1</a>] [<a
      href="https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e"
      target="_blank">link2</a>].</p></li>

      <li><p>In a confusion matrix, the acceptable level of class imbalance is
      not strictly defined and can vary significantly depending on the
      specific context and domain of the application. A balanced dataset is
      one where each class has approximately the same number of instances, but
      this is not the case in many real-world scenarios, like in this project.
      In practice, an imbalance ratio of 1:10 is often considered moderately
      imbalanced, while 1:100 or greater is highly imbalanced.</p></li>

      <li><p>When classes are imbalanced, accuracy becomes a less reliable
      metric. In such cases, other metrics like precision, recall, F1 score,
      and the area under the receiver operating characteristic curve (AUC-ROC)
      become more important to evaluate performance correctly.</p></li>

      <li><p>A guide for tuning hyperparameters in BERT models is available
      here: <a href="bert-hyperparameters.html">BERT Hyperparamters: A Guide
      to Fine Tune BERT models</a>.</p></li>

      <li><p>There is an option to ignore segments of the dataset according to
      the annotator. For instance, you can exclude Mr Reyes's annotations in
      this part of the code in this way:</p><pre>if passage["metadata"]["annotator"] in ["IE-Reyes"]:
  continue</pre><p>You can add someone else in the list to extend the ignore
      block:</p><pre>if passage["metadata"]["annotator"] in ["IE-Reyes", "IE-Doe"]:
  continue</pre><p>In this way, you remove segments of the dataset that, for
      some reason, you consider they are adding noise to the model. Always
      ignore Mr Reyes's datapoints as implemented in the code.</p></li>
    </ol><h3>Grading Schema</h3><table>
      <thead>
        <tr>
          <th><strong>Area</strong></th>

          <th><strong>Task</strong></th>

          <th><strong>Description</strong></th>

          <th><strong>Point weight</strong></th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td rowspan="2">Dataset</td>

          <td>Annotation</td>

          <td>Quality, completion of annotation:<ul>
              <li><p>1-3 wrong passages: 2.70.</p></li>

              <li><p>4-6 wrong passages: 2.40.</p></li>

              <li><p>7-9 wrong passages: 2.10.</p></li>

              <li><p>10-12 wrong passages: 1.80.</p></li>

              <li><p>13-15 wrong passages: 1.50.</p></li>

              <li><p>16-18 wrong passages: 1.20.</p></li>

              <li><p>19-20 wrong passages: 0.90.</p></li>

              <li><p>21-22 wrong passages: 0.60.</p></li>

              <li><p>23-24 wrong passages: 0.30.</p></li>

              <li><p>+25 wrong passages: 0.</p></li>
            </ul></td>

          <td>3</td>
        </tr>

        <tr>
          <td>Peer review</td>

          <td>Quality, completion of annotation in assigned dataset:<ul>
              <li><p>1-3 wrong passage3: 1.80.</p></li>

              <li><p>4-6 wrong passages: 1.60.</p></li>

              <li><p>7-9 wrong passages: 1.40.</p></li>

              <li><p>10-12 wrong passages: 1.20.</p></li>

              <li><p>13-15 wrong passages: 1.00.</p></li>

              <li><p>16-18 wrong passages: 0.80.</p></li>

              <li><p>19-20 wrong passages: 0.60.</p></li>

              <li><p>21-22 wrong passages: 0.40.</p></li>

              <li><p>23-24 wrong passages: 0.20.</p></li>

              <li><p>+25 wrong passages: 0.</p></li>
            </ul></td>

          <td>2</td>
        </tr>

        <tr>
          <td rowspan="3">Model performance</td>

          <td>Metrics (*)</td>

          <td>Accuracy above 0.750</td>

          <td>2</td>
        </tr>

        <tr>
          <td>Confusion matrix (*)</td>

          <td><p>Moderated imbalance ratio (&lt;=1:20)</p></td>

          <td>1.25</td>
        </tr>

        <tr>
          <td>Training/Validation Loss (*)</td>

          <td>The model is neither overfitted not underfitted, according to
          visualizations (see readings above).</td>

          <td>1.25</td>
        </tr>

        <tr>
          <td>Participation</td>

          <td></td>

          <td>Communication on the forum and collaborative work.</td>

          <td>0.5</td>
        </tr>

        <tr>
          <td>Late submission</td>

          <td></td>

          <td>Deducted points per day.</td>

          <td>-0.2</td>
        </tr>

        <tr>
          <td><strong>Total</strong></td>

          <td></td>

          <td></td>

          <td><strong>10</strong></td>
        </tr>
      </tbody>
    </table></section><section><p>(*) Each grading item in Model performance
  must be accomplished in conjunction with the others to get its maximal
  grade. For instance, if accuracy is higher than 0.750 but there is a high
  imbalance in the confusion matrix and the training/validation losses
  visualization shows overfitting, only 0.75 points will be given to the three
  items, according to this table:</p><table border="1">
      <tr>
        <td><strong>Case 1</strong></td>

        <td><strong>Case 2</strong></td>

        <td><strong>Case 3</strong></td>
      </tr>

      <tr>
        <td>1 item achieved + 2 items not achieved</td>

        <td>2 items achieved + 1 item not achieved</td>

        <td>3 items achieved</td>
      </tr>

      <tr>
        <td><kbd>0.25 x 3 = 0.75</kbd> points</td>

        <td><kbd>0.5 x 3 = 1.5</kbd> points</td>

        <td><kbd>5</kbd> points</td>
      </tr>
    </table><p></p><h3>Model Baseline</h3><p>The provided code has the
  following hyperparameter values:</p><ul>
      <li>LEARNING_RATE = 1.5e-5</li>

      <li>BATCH_SIZE = 16</li>

      <li>WARMUP_STEPS = 100</li>

      <li>NUM_EPOCHS = 3</li>

      <li>WEIGHT_DECAY = 2e-2</li>

      <li>DROP_OUT_RATE = 0.2</li>
    </ul><p>Resulting metrics: </p><ol>
      <li>Accuracy: 0.649</li>

      <li>Precision: 0.655</li>

      <li>Recall: 0.648</li>

      <li>F1 Score: 0.644</li>

      <li>AUC-ROC: 0.704</li>

      <li>Matthews Correlation Coefficient (MCC): 0.303</li>

      <li>Confusion Matrix:<table border="1">
          <tr>
            <td></td>

            <td>continue</td>

            <td>not_continue</td>
          </tr>

          <tr>
            <td>continue</td>

            <td>221</td>

            <td>72</td>
          </tr>

          <tr>
            <td>not_continue</td>

            <td>132</td>

            <td>156</td>
          </tr>
        </table></li>

      <li>Main metrics per class:<table border="1">
          <tr>
            <td>continue</td>

            <td>Precision = 0.59</td>

            <td>Recall = 0.52</td>

            <td>F1 = 0.55</td>
          </tr>

          <tr>
            <td>not_continue</td>

            <td>Precision = 0.56</td>

            <td>Recall = 0.64</td>

            <td>F1 = 0.60</td>
          </tr>
        </table></li>
    </ol><p>Training/Validation losses:</p><figure><img
  alt="Visualization of training/validation losses."
  src="images/baseline_model_losses.png" width="700" />
  <figcaption>Visualization of training/validation losses.
  </figcaption></figure></section><section><p>In a good fit model, both
  training and validation losses should decrease over time as the model learns
  from the data. If the validation loss starts to increase while the training
  loss continues to decrease, it may indicate overfitting, meaning the model
  is learning the training data too closely and not generalizing well to new,
  unseen data.</p><p>The visualization shows the training and validation
  losses over three epochs initially closely aligned while decreasing,
  indicating that the model is learning and improving its performance and not
  overfitting to the training data. After the second epoch, where the lines
  cross, the training loss continues to decrease, while the validation loss
  starts to increase. This phenomenon typically indicates that the model is
  beginning to overfit to the training data.</p><p>Accuracy (0.649): About
  64.9% of the predictions are correct.</p><p>Precision (0.655) and Recall
  (0.648): Precision indicates that when the model predicts a class, it is
  correct about 65.5% of the time. Recall indicates that the model correctly
  identifies 64.8% of all actual instances of the classes.</p><p>The Area
  Under the Receiver Operating Characteristic (AUC-ROC) curve is above 0.7,
  suggesting that the model has a good measure of separability. It can
  distinguish between the classes to a good extent.</p><p>Matthews Correlation
  Coefficient (MCC) (0.303): The MCC is low, indicating that the model's
  predictions are not highly correlated with the actual values. MCC is a
  balanced measure even if the classes are of very different
  sizes.</p><p>Confusion Matrix: The confusion matrix shows that the model is
  better at predicting 'continue' correctly than 'not_continue'. However,
  there is a significant number of false positives and false negatives for
  both classes.</p><h3>Submission of Project 2's Results</h3><p>Submit the
  following items included in a single zip file:</p><ol>
      <li><p><kbd>build_dataset_baseline.py</kbd> file.</p></li>

      <li><kbd>dl_bert_train.py</kbd> file.</li>

      <li><p><kbd>dataset2_model_confusion_matrix.png</kbd> file.</p></li>

      <li><kbd>bert_model_2_losses.png</kbd> file.</li>

      <li><p><kbd>bert_model_2_metrics.txt</kbd> file with all metrics
      information.</p></li>
    </ol></section></main></section><footer><p>---</p><p><small>This
  documentation was developed by Juan-Francisco Reyes for the seminar on
  Information Extraction from Web Resources at the Brandenburgische Technische
  Universität Cottbus-Senftenberg in the Winter semester of
  2023/24.</small></p> <small>© 2024 Juan-Francisco Reyes</small>
  </footer></body>
</html>
