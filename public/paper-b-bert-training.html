<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="UTF-8" />

    <meta content="width=device-width,initial-scale=1" name="viewport" />

    <link href="styles.css" rel="stylesheet" />

    <title>IE Seminar - Fine-tuning a BERT model</title>
  </head>

  <body><header> <a href="index.html"> « IE Seminar</a> </header><section><div
  class="main-title"><h1>BERT Model 2: Fine-Tuning Deep-Learning Model for
  Passage Boundary Detection</h1><p><strong>Institute of Computer Science,
  Brandenburgische Technische Universität Cottbus-Senftenberg</strong><br />
  Juan-Francisco Reyes<br />
  pacoreyes@protonmail.com</p></div><main><section><p style="color: red">###
  THIS PAGE WILL BE UPDATED PERMANENTLY BASED ON INTERACTIONS ON THE FORUM,
  RETURN OFTEN</p><h3>Overview</h3><p>This document delineates the process of
  preprocessing Dataset 2 and fine-tuning a BERT model for binary text
  classification.</p><h3>Challenges in Passage Boundary
  Detection</h3><p>Passage Boundary Detection (PBD) refers to identifying the
  boundaries between distinct passages or segments within a text, which is
  essential for tasks such as document summarization, information retrieval,
  and text segmentation, where understanding the structure of the text is
  crucial. The main challenges in PSB are:</p><ol>
      <li><p><strong>Definition agreement</strong>: There is no universal
      agreement on what constitutes a "passage," as its definition can vary
      significantly depending on the type of text (e.g., literary texts,
      scientific articles, news reports, political discourse) and the specific
      application. This variability adds complexity to designing algorithms
      that can universally detect passage boundaries.</p></li>

      <li><p><strong>Ambiguity</strong>: Unlike sentences often delimited by
      clear punctuation marks, passage boundaries can be vague and unclear.
      Identifying where one passage ends and another begins requires
      understanding the linguistic features that link sentences, context, the
      topic, and sometimes the subtle topic shifts.</p></li>

      <li><p><strong>Continuity</strong>: Passages may have thematic or
      contextual overlaps, making determining where one topic ends and another
      begins is challenging. In fine-grained PBD, distinguishing these subtle
      changes in topic or perspective requires a sophisticated understanding
      and analysis of the text.</p></li>

      <li><p><strong>Variability</strong>: Variations in writing style,
      language use, and narrative techniques across different authors and text
      types can impact the effectiveness of PBD algorithms. Detecting
      boundaries in more stylistically complex or linguistically diverse texts
      can be particularly challenging.</p></li>

      <li><p><strong>Advanced NLP</strong>: Effective fine-grained PBD often
      requires advanced NLP techniques to understand nuanced language patterns
      and contextual cues. However, overall, these techniques require mainly
      well-annotated datasets for training.</p></li>

      <li><p><strong>Linguistic analysis</strong>: Understanding coherence
      (logical flow and connection between ideas) and cohesion (the
      grammatical and lexical linking within a text) is vital for effective
      PBD, allowing the model to process the text at a surface level and
      understand deeper linguistic and semantic structures.</p></li>
    </ol><h3>Annotation Approach: Pair Labelling</h3><p>This approach focuses
  on analyzing and annotating pairs of sentences to determine how they relate
  to each other in terms of continuity and coherence. This process is
  fundamental in understanding the flow of ideas and logical progression in a
  text.</p><p>Typically, the annotation schema includes binary labels like
  "Same Topic" and "Topic Change". Annotators review pairs of sentences and
  assign one of these labels based on the thematic continuity or
  discontinuity. We will use the labels "continue" and
  "not_continue".</p><p>Example:</p><ol>
      <li><p><strong>[not_continue]</strong> And I want to thank Mike. Today,
      it's a true honor to be the first President of the United States to host
      a meeting at the United Nations on religious freedom.</p></li>

      <li><p><strong>[continue]</strong> Today, it's a true honor to be the
      first President of the United States to host a meeting at the United
      Nations on religious freedom. And an honor it is. It's long
      overdue.</p></li>

      <li><p><strong>[continue]</strong> And an honor it is. It's long
      overdue. And I was shocked when I was given that statistic that I would
      be the first.</p></li>

      <li>(...)</li>
    </ol><p>The core idea is to analyze pairs of adjacent sentences and label
  them based on whether they continue the same topic or start a new one. This
  binary classification (same topic vs. new topic) helps identify the points
  where the subject matter shifts, thereby detecting topic
  boundaries.</p><p>In this project, the annotation will be made automatically
  from the annotated passages from Dataset 2. </p><h3>Files and folders
  structure</h3><p>This is the list of provided files:</p><ul>
      <li><p><kbd>build_dataset2_baseline.py</kbd>: Script to convert
      annotated data into dataset 2 for the pair sentence labelling
      approach.</p></li>

      <li><p><kbd>bert_model_2_training.py</kbd>: Script to fine-tune a BERT
      model for sentence boundary detection using the pair sentence labelling
      approach.</p></li>
    </ul><h3>"Feature Engineering" and BERT Models</h3><p>In classical machine
  learning models, feature engineering is a critical step involving selecting
  manually and transforming raw data into a set of features that the model can
  understand and process effectively. In contrast, Transformer models,
  including BERT, are designed to automatically learn feature representations
  from raw text data through multiple layers of self-attention and feedforward
  neural networks. This process allows the model to learn complex
  representations of the input text without explicit manual intervention in
  the feature extraction process.</p><p>While fine-tuning BERT models does not
  entail manually selecting or creating features, it involves selecting
  appropriate datasets that are representative of the task at hand. For NLP
  projects, this involves understanding linguistic structures and features in
  the classification challenge. The diversity, representation, and quantity of
  datapoints are crucial; thus, curating a dataset that effectively represents
  the problem space and contains various examples for the model to learn from
  is essential. Additionally, converting raw text into a format for BERT
  models is another critical step. This process involves tokenization
  (breaking text into tokens), adding special tokens (like <kbd>[CLS]</kbd>
  and <kbd>[SEP]</kbd>), and creating attention masks. Finally, determining
  appropriate hyperparameters (such as learning rate, batch size, etc.) can
  also be considered a part of feature engineering in this context, which
  involves tuning the model to fit the specific characteristics of the data
  and the task better.</p><p>While feature engineering in the traditional
  sense is not a primary concern when working with BERT and similar models,
  the selection and preparation of data, along with model configuration and
  hyperparameter tuning, play a crucial role in the fine-tuning process. These
  steps ensure the model learns the most relevant features from the data for
  the specific NLP task.</p><h3>Before You Begin</h3><p>Do "feature
  engineering" by completing first the peer review of the dataset according to
  the table of reviewer/reviewed. Every improvement in the dataset will impact
  the overall result. For this purpose, you will follow these steps:</p><ol
      type="a">
      <li><p>Complete a last review of the annotation of passages ASAP, and
      contact your reviewers to let them know when they may start their peer
      review task. You can also coordinate with your reviewer to do reviews in
      batches according to your progress - that requires high organization
      between parties. <span style="color: red">The deadline for the peer
      review has been extended from December 8 to December 14 at 10:00. From
      that point, Dataset 2 will be downloaded and the grading process for
      dataset building (3 points) and dataset peer review (2 points) will
      begin; therefore, no change should be made to the dataset
      afterward</span>.</p></li>

      <li><p>Every wrong passage found in the peer review must be flagged as
      "to reject" in the IIA document, and the annotator (not the reviewer)
      must correct the mistake or reject it. Recall that the model accuracy
      improves whenever a wrong passage is corrected or rejected. Follow what
      Mr Urrego has done by logging and rejecting wrong datapoints in Mr
      Shresda's annotation on the shared Google Spreadsheet.</p></li>

      <li><p>Announce on the forum when you finish your peer-review task to
      let your classmates know that an improved version of Dataset 2 is
      available to download. Effective communication and collaborative work
      are crucial and will be evaluated in Project 2's individual grade. See
      grading schema.</p></li>
    </ol><p>In the real world, building a dataset is a collaborative project;
  therefore, any individual contribution to improve the dataset will impact
  the model's performance and, in this case, the grade of every participant.
  Work collaboratively!!</p><p>Notice that in this project, we are trying to
  capture nuanced linguistic features that define topic shifts in political
  discourse. Likely, the number of examples (datapoints) to fine-tune a BERT
  model will not be enough to get a high performance. For this reason, in this
  complex project, we are not going to be very ambitious with performance
  metrics.</p><h3>Procedure</h3><ol>
      <li>Recall that the two factors in which you have full control and have
      the higher impact on the final results are the dataset quality and
      training hyperparameters. Therefore, don't expect a higher performance
      in your model if your annotation work is poor.</li>

      <li><p>Download the different versions of Dataset 2 JSONL file. Each
      version represents a different state of the dataset along the
      improvement process:</p><ul>
          <li><p><kbd><a download="dataset2_raw_dec_11.jsonl"
          href="datasets/dataset2_raw_dec_11.jsonl">dataset2_raw_dec_11.jsonl</a></kbd>,
          baseline (Dec 11).</p></li>

          <li><p><kbd><a download="dataset2_raw_dec_13a.jsonl"
          href="datasets/dataset2_raw_dec_13a.jsonl">dataset2_raw_dec_13a.jsonl</a></kbd>
          (Dec 13 at 16:00). </p></li>

          <li><p><kbd><a download="dataset2_raw_dec_13b.jsonl"
          href="datasets/dataset2_raw_dec_13b.jsonl">dataset2_raw_dec_13b.jsonl</a></kbd>
          (Dec 13 at 22:00).</p></li>

          <li><p><kbd><a download="dataset2_raw_dec_16.jsonl"
          href="datasets/dataset2_raw_dec_16.jsonl">dataset2_raw_dec_16.jsonl</a></kbd>
          before grading (Dec 16).</p></li>

          <li><p><kbd><a download="dataset2_pair_sentences_final.jsonl"
          href="dataset2_pair_sentences_final.jsonl">dataset2_pair_sentences_final.jsonl</a></kbd>
          after grading and corrections by the lecturer (Dec 19). This version
          does not need to be preprocessed using the script
          <kbd><kbd>build_dataset2_baseline.py</kbd></kbd>.</p></li>
        </ul></li>

      <li><p>Use the script <kbd><kbd>build_dataset2_baseline.py</kbd></kbd>
      to generate the annotated dataset for the sentence pair labelling
      annotation approach. The text will be automatically anonymized. Adjust
      the name of the input dataset downloaded from the links above. After
      processing the dataset, a new version of your dataset with the name
      <kbd>dataset2_pair_sentences.jsonl</kbd> will be generated containing
      pairs of sentences.</p></li>

      <li>Study the model baseline and grading schema.</li>

      <li><p>Use the training script <kbd>bert_model_2_training.py</kbd>.
      Modify hyperparameters iteratively to achieve the highest accuracy
      possible of the model. Use the different versions of the dataset to see
      the evolution of Dataset 2 and the BERT model. Register the evolution of
      results for the final presentation. </p></li>

      <li><p>Use the visualizations to evaluate the performance of the model.
      The <a
      href="https://www.evidentlyai.com/classification-metrics/confusion-matrix"
      target="_blank">confusion matrix</a> visualization
      (<kbd>bert_model_2_confusion_matrix.png</kbd>) helps you to evaluate the
      performance of the model by visualizing False and True Positives and
      Negatives in two dimensions, "Actual" and "Predicted". The training and
      validation losses visualization (<kbd>bert_model_2_losses.png</kbd>) to
      follow up if your model is overfitting or underfitting [<a
      href="https://medium.com/nlplanet/bert-finetuning-with-hugging-face-and-training-visualizations-with-tensorboard-46368a57fc97"
      target="_blank">link1</a>] [<a
      href="https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e"
      target="_blank">link2</a>].</p></li>

      <li>Implement a mechanism to stop the training process when the
      validation loss exceeds the training loss, indicating overfitting.</li>

      <li><p>A guide for tuning hyperparameters in BERT models is available
      here: <a href="bert-hyperparameters.html">BERT Hyperparamters: A Guide
      to Fine Tune BERT models</a>.</p></li>

      <li><p>There is an option to ignore segments of the dataset according to
      the annotator. For instance, you can exclude Mr Reyes's annotations in
      this part of the code in this way:</p><pre><span style="color: white">if passage["metadata"]["annotator"] in ["IE-Reyes"]:
  continue</span></pre><p>You can add someone else in the list to extend the
      ignore block:</p><pre><span style="color: white">if passage["metadata"]["annotator"] in ["IE-Reyes", "IE-Doe"]:
  continue</span></pre><p>In this way, you remove segments of the dataset
      that, for some reason, you consider they are adding noise to the model.
      Always ignore Mr Reyes's datapoints as implemented in the code.</p></li>
    </ol><h3>Grading Schema</h3><table>
      <thead>
        <tr>
          <th><strong>Area</strong></th>

          <th><strong>Task</strong></th>

          <th><strong>Description</strong></th>

          <th><strong>Point weight</strong></th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td rowspan="2">Dataset</td>

          <td>Annotation</td>

          <td>Quality, completion of annotation:<ul>
              <li><p>1-3 wrong passages: 2.70.</p></li>

              <li><p>4-6 wrong passages: 2.40.</p></li>

              <li><p>7-9 wrong passages: 2.10.</p></li>

              <li><p>10-12 wrong passages: 1.80.</p></li>

              <li><p>13-15 wrong passages: 1.50.</p></li>

              <li><p>16-18 wrong passages: 1.20.</p></li>

              <li><p>19-20 wrong passages: 0.90.</p></li>

              <li><p>21-22 wrong passages: 0.60.</p></li>

              <li><p>23-24 wrong passages: 0.30.</p></li>

              <li><p>+25 wrong passages: 0.</p></li>
            </ul></td>

          <td>3</td>
        </tr>

        <tr>
          <td>Peer review</td>

          <td>Quality, completion of annotation in assigned dataset:<ul>
              <li><p>1-3 wrong passage3: 1.80.</p></li>

              <li><p>4-6 wrong passages: 1.60.</p></li>

              <li><p>7-9 wrong passages: 1.40.</p></li>

              <li><p>10-12 wrong passages: 1.20.</p></li>

              <li><p>13-15 wrong passages: 1.00.</p></li>

              <li><p>16-18 wrong passages: 0.80.</p></li>

              <li><p>19-20 wrong passages: 0.60.</p></li>

              <li><p>21-22 wrong passages: 0.40.</p></li>

              <li><p>23-24 wrong passages: 0.20.</p></li>

              <li><p>+25 wrong passages: 0.</p></li>
            </ul></td>

          <td>2</td>
        </tr>

        <tr>
          <td rowspan="3">Model performance</td>

          <td>Metrics (*)</td>

          <td>Accuracy above 0.860</td>

          <td>2</td>
        </tr>

        <tr>
          <td>Confusion matrix (*)</td>

          <td><p>Moderated imbalance ratio (&lt;=2:10)</p></td>

          <td>1.25</td>
        </tr>

        <tr>
          <td>Training/Validation Loss (*)</td>

          <td>The model is neither overfitted not underfitted, according to
          visualizations (see readings above). A small amount of overfitting
          is acceptable.</td>

          <td>1.25</td>
        </tr>

        <tr>
          <td>Participation</td>

          <td></td>

          <td>Communication on the forum and collaborative work.</td>

          <td>0.5</td>
        </tr>

        <tr>
          <td>Late submission</td>

          <td></td>

          <td>Deducted points per day.</td>

          <td>-0.2</td>
        </tr>

        <tr>
          <td><strong>Total</strong></td>

          <td></td>

          <td></td>

          <td><strong>10</strong></td>
        </tr>
      </tbody>
    </table></section><section><p>(*) Each grading item in Model performance
  must be accomplished in conjunction with the others to get its maximal
  grade. For instance, if accuracy is higher than 0.750 but there is a high
  imbalance in the confusion matrix and the training/validation losses
  visualization shows overfitting, only 0.75 points will be given to the three
  items, according to this table:</p><table border="1">
      <tr>
        <td><strong>Case 1</strong></td>

        <td><strong>Case 2</strong></td>

        <td><strong>Case 3</strong></td>
      </tr>

      <tr>
        <td>1 item achieved + 2 items not achieved</td>

        <td>2 items achieved + 1 item not achieved</td>

        <td>3 items achieved</td>
      </tr>

      <tr>
        <td><kbd>0.25 x 3 = 0.75</kbd> points</td>

        <td><kbd>0.5 x 3 = 1.5</kbd> points</td>

        <td><kbd>5</kbd> points</td>
      </tr>
    </table><h3>Model Baseline</h3><p>This baseline corresponds to
  dataset2_raw_dec_11.jsonl, and it includes the annotations of Ms Arias, Ms
  Abanda, Ms Joseph, Mr Arayan, Mr Shrestha, and Mr Urrego; it does not
  include the segment of Mr Asgola since, at that time, his continuation in
  the seminar was not confirmed. I used the ignore segment option described
  above located in the script <kbd>build_dataset2_baseline.py</kbd> to exclude
  his annotation progress.</p><p>The provided code has the following
  hyperparameter values:</p><ul>
      <li>LEARNING_RATE = 1.2e-5</li>

      <li>BATCH_SIZE = 16</li>

      <li>WARMUP_STEPS = 1000</li>

      <li>NUM_EPOCHS = 3</li>

      <li>WEIGHT_DECAY = 1e-3</li>

      <li>DROP_OUT_RATE = 0.1</li>
    </ul><p>Resulting metrics: </p><ol>
      <li>Accuracy: 0.616</li>

      <li>Precision: 0.616</li>

      <li>Recall: 0.616</li>

      <li>F1 Score: 0.616</li>

      <li>AUC-ROC: 0.690</li>

      <li>Matthews Correlation Coefficient (MCC): 0.232</li>

      <li>Confusion Matrix:<table border="1">
          <tr>
            <td></td>

            <td>continue</td>

            <td>not_continue</td>
          </tr>

          <tr>
            <td>continue</td>

            <td>190</td>

            <td>103</td>
          </tr>

          <tr>
            <td>not_continue</td>

            <td>120</td>

            <td>168</td>
          </tr>
        </table></li>

      <li>Main metrics per class:<table border="1">
          <tr>
            <td>continue</td>

            <td>Precision = 0.55</td>

            <td>Recall = 0.62</td>

            <td>F1 = 0.58</td>
          </tr>

          <tr>
            <td>not_continue</td>

            <td>Precision = 0.56</td>

            <td>Recall = 0.49</td>

            <td>F1 = 0.52</td>
          </tr>
        </table></li>
    </ol><p>Training/Validation losses:</p><figure><img
  alt="Visualization of training/validation losses."
  src="images/baseline_model_losses.png" width="700" />
  <figcaption>Visualization of training/validation losses.
  </figcaption></figure></section><section><h4>Before you begin</h4><p>In a
  good fit model, both training and validation losses should decrease over
  time as the model learns from the data. </p><ul>
      <li><p><strong>Training Loss</strong>: This is the error on the training
      split, which should decrease steadily as the model learns from the
      training data.</p></li>

      <li><p><strong>Validation Loss</strong>: This is the error on a separate
      dataset not seen by the model during training, used to evaluate the
      model's generalization capability.</p></li>
    </ul><p>If the validation loss starts to increase while the training loss
  continues to decrease, it may indicate overfitting, meaning the model is
  learning the training data too closely and not generalizing well to new,
  unseen data. Also, it may indicate data leakage strong regularization or
  that the model capacity is not fully utilized; that is, the model could be
  underfitting the training set.</p>In a confusion matrix, the acceptable
  level of class imbalance is not strictly defined and can vary significantly
  depending on the specific context and domain of the application. A balanced
  dataset is one where each class has approximately the same number of
  instances, but this is not the case in many real-world scenarios, like in
  this project. In practice, an imbalance ratio of 1:10 is often considered
  moderately imbalanced, while 1:100 or greater is highly
  imbalanced.<p>Reproducibility in NLP ensures that the results of a model can
  be consistently achieved by different developers, across various
  computational environments, and at different times.</p><h4>Results
  analysis</h4><p>The visualization shows the training and validation losses
  over three epochs initially closely aligned while decreasing, indicating
  that the model is learning and improving its performance and not overfitting
  to the training data. After the middle of the second epoch, where the lines
  cross, the training loss decreases while the validation loss increases. This
  phenomenon typically indicates that the model is beginning to overfit the
  training data.</p><p>The model's accuracy is 0.616, indicating that
  approximately 61.6% of its predictions align with the ground truth. Notably,
  the model's precision and recall also manifest as 0.616. Precision at this
  level suggests that, of all instances classified positively by the model,
  61.6% are indeed positive according to the actual labels. Simultaneously,
  the equivalent recall value indicates that the model successfully identifies
  61.6% of the total true positive cases in the dataset. The confluence of
  accuracy, precision, and recall at the same metric value is atypical,
  potentially signifying a balanced distribution of classes within the
  dataset. Moreover, it implies a symmetrical performance by the model across
  both classes—neither favoring nor discriminating against any particular
  class.</p><p>An F1 score of 0.616 emerges from the harmonic mean of
  precision and recall, indicating a balanced trade-off between these metrics.
  Unlike an arithmetic mean, the harmonic mean penalizes extreme values more
  severely; thus, the F1 score is a more robust measure when dealing with
  imbalanced datasets. However, in this context, where precision and recall
  are equal, the F1 score merely mirrors these values, encapsulating the
  model's consistent but moderate efficacy across its predictive capabilities.
  While these observations suggest uniformity in the model's predictive
  performance, the moderate level of these metrics indicates substantial room
  for improvement. Specifically, enhancing the F1 score would require a
  focused effort to concurrently elevate both precision and recall, thereby
  achieving a more refined balance between the model's ability to accurately
  predict positive instances and its capacity to capture the majority of
  actual positives.</p><p>The Area Under the Receiver Operating Characteristic
  (AUC-ROC) curve is circa 0.7, suggesting that the model has a good
  separability measure, but further improvements are necessary. It can
  distinguish between the classes to a reasonable extent. However, the
  Matthews Correlation Coefficient (MCC) (0.232) is low, indicating that the
  model's predictions are not highly correlated with the actual values. MCC is
  a balanced measure even if the classes are of very different sizes. Consider
  MCC as a more reliable statistical rate, which produces a high score only if
  the prediction obtained good results in all four confusion matrix
  categories, proportional to both class sizes. The low score indicates that
  the model performs poorly in distinguishing between classes. You will see an
  increase in this score when the model better understands the separation
  between classes.</p><p>The confusion matrix shows that the model predicts
  "continue" better than "not_continue". However, both classes have a
  significant number of false positives and false negatives.</p><p>In the
  current state of the dataset and model, you will get different results
  whenever you run the script. This reproducibility issue indicates that the
  dataset has many misclassifications. If noise is introduced into the
  dataset, it will result in high variability in model performance across
  different runs.</p><p>While we eliminate misclassification, we will see an
  immediate impact in all metrics, meaning better class separations; however,
  we should pay close attention to MMC and the balance of classes in the
  confusion matrix to evaluate improvements.</p><h3>Submission of Project 2's
  Results</h3><p>Submit the following items included in a single zip
  file:</p><ol>
      <li><p><kbd>build_dataset2_baseline.py</kbd> file.</p></li>

      <li><kbd>bert_model_2_training.py</kbd> file.</li>

      <li><p><kbd>bert_model_2_confusion_matrix.png</kbd> file (5 versions, 1
      per iteration/dataset version).</p></li>

      <li><kbd>bert_model_2_losses.png</kbd> file (5 versions, 1 per
      iteration/dataset version).</li>

      <li><p><kbd>bert_model_2_metrics_and_hyperparameters.txt</kbd> file with
      all metrics and hyperparameters information (5 versions, 1 per
      iteration/dataset version).</p></li>
    </ol></section><section><p>Create individual folders for the 5
  iterations:</p><ul>
      <li><p><kbd>run1</kbd></p></li>

      <li><p><kbd>run2</kbd></p></li>

      <li><p><kbd>run3</kbd></p></li>

      <li><p><kbd>run4</kbd></p></li>

      <li><p><kbd>run5</kbd></p></li>
    </ul><h3>Presentation Session</h3><p>On the session of December 19, you
  will present your results showing the evolution of 5 runs of your
  fine-tuning process using 5 versions of the raw dataset 2. In the
  presentations, participants will make an analysis of their results,
  including individual metrics, visualizations, and changes between different
  versions of the dataset.</p><p>The presentation includes the explanation of
  the early stop mechanism implemented in the
  code.</p></section></main></section><footer><p>---</p><p><small>This
  documentation was developed by Juan-Francisco Reyes for the seminar on
  Information Extraction from Web Resources at the Brandenburgische Technische
  Universität Cottbus-Senftenberg in the Winter semester of
  2023/24.</small></p> <small>© 2024 Juan-Francisco Reyes</small>
  </footer></body>
</html>
